<img width="647" height="513" alt="image" src="https://github.com/user-attachments/assets/14c64525-8cef-44f0-b17c-b83ab7815724" />
# Projeto Indicium Data Pipeline

Este projeto implementa um pipeline de dados para extrair informa√ß√µes de um banco de dados PostgreSQL (Northwind) e um arquivo CSV (`order_details`), salv√°-los no disco local e, em seguida, carreg√°-los em um banco de dados PostgreSQL de destino. A orquestra√ß√£o √© feita com Apache Airflow e a movimenta√ß√£o de dados com Meltano, tudo conteinerizado com Docker Compose.

---

## üìå Tecnologias Utilizadas

- **Apache Airflow**: Orquestra√ß√£o de workflows.
- **Meltano**: Extra√ß√£o e carregamento de dados (ELT).
- **PostgreSQL**: Banco de dados fonte e destino.
- **Docker & Docker Compose**: Conteineriza√ß√£o e orquestra√ß√£o.
- **WSL2**: Ambiente de desenvolvimento.

---

## ‚öôÔ∏è Pr√©-requisitos

Certifique-se de ter instalado:

- Docker Desktop (com backend WSL2 habilitado)
- WSL2 (com uma distribui√ß√£o Linux instalada)
- Git
- Make

---

## üöÄ Como Rodar o Projeto

### 1. Clonar o Reposit√≥rio

```bash
git clone https://github.com/roboberto1403/indicium_pipeline.git
cd indicium_pipeline
```
### 2. Configura√ß√£o Inicial
Antes de iniciar, crie ou edite o arquivo .env na raiz do projeto com as seguintes vari√°veis:

```bash
POSTGRES_USER_DESTINO=seu_usuario_destino
POSTGRES_PASSWORD_DESTINO=sua_senha_destino
POSTGRES_DB_DESTINO=seu_banco_destino
```
Para configurar o ambiente automaticamente, execute:

```bash
make setup
```
Esse comando executa o script setup.sh, que detecta o IP do host e configura os arquivos .env necess√°rios para o projeto e o Meltano.

### 3. Iniciar os Servi√ßos

```bash
make start
```
Internamente, esse comando executa:

```bash
sudo docker compose up -d --build --force-recreate
```
Isso iniciar√° os cont√™ineres do PostgreSQL (fonte e destino) e do Airflow. O banco db_source ser√° populado automaticamente com northwind.sql na primeira execu√ß√£o (ou ap√≥s remo√ß√£o dos volumes).

Aguarde alguns minutos at√© que todos os servi√ßos estejam prontos.

### 4. Acessar e Executar o Pipeline no Airflow
Acesse a interface do Airflow: 

üîó http://localhost:8080

- Login: `airflow`
- Senha: `airflow`

Na UI do Airflow:

1. Localize o DAG `northwind`
2. Ative o DAG (toggle switch)
3. Clique em "Play" (Trigger DAG)

## üîÅ Diagrama do Fluxo ELT
O fluxo completo do pipeline de dados pode ser visualizado a seguir:

<img width="647" height="513" alt="image" src="https://github.com/user-attachments/assets/14c64525-8cef-44f0-b17c-b83ab7815724" />

### 5. Verifica√ß√£o e Evid√™ncia

## üìÅ Verifique os dados locais
Os arquivos .parquet estar√£o organizados da seguinte forma:

```bash
/data/postgres/{table}/{YYY-MM-DD}/file.csv
/data/csv/{table}/{YYY-MM-DD}/file.csv
```

## üßæ Uma evid√™ncia √© gerada na √∫ltima etapa da DAG (validate_data_in_postgres), a qual roda o seguinte comando SQL:

```bash
SELECT
                o.order_id AS ID_Pedido,
                o.order_date AS Data_Pedido,
                o.customer_id AS ID_Cliente,
                od.product_id AS ID_Produto,
                od.unit_price AS Preco_Unitario,
                od.quantity AS Quantidade,
                od.discount AS Desconto
            FROM
                orders o
            JOIN
                order_details od ON o.order_id = od.order_id
```
Os dados advindos da tarefa podem ser conferidos na aba XCom 

### 6. Parar e Limpar os Servi√ßos

Para parar os servi√ßos:
```bash
make down
```
Para parar e remover volumes/dados completamente:
```bash
make clean
```

### üì¨ Contato

Caso tenha d√∫vidas ou sugest√µes, fique √† vontade para me contatar pelo meu email [lrbf@cin.ufpe.br](mailto:lrbf@cin.ufpe.br). 
